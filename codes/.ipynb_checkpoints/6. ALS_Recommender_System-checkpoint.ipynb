{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menu Recommender Using Alternating Least Squares (ALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I build a recommender system to support upselling, using the collaborative filtering approach which allows us to predict the interests of a user by collecting preferences or taste information from many other users.\n",
    "\n",
    "The data we have is purchase data which is implicit feedback (where user-item interaction consists of positive only preferences) compared to ratings which are considered as explicit feedback. I use the Alternating Least Squares(ALS) which is particularly useful for implicit feedback.\n",
    "\n",
    "We are taking a matrix of user-item interactions and figuring out the latent features that relate them to each other. This matrix factorisation method reduces the dimensions/ features (while keeping relevant information) into a smaller matrix of user features and item features.\n",
    "\n",
    "The matrix factorisation results is:\n",
    "1. One smaller matrix with dimensions: num of users * latent feature --> contains latent user feature vectors for each user\n",
    "2. And another matrix with dimensions: num of items * latent feature --> contains latent item feature vectors for each item\n",
    "\n",
    "Multiplying these two feature matrices together approximates the original matrix, but now we have two matrices that are dense including a number of latent features for each of our items and users.\n",
    "\n",
    "Reference taken from: https://nbviewer.jupyter.org/github/jmsteinw/Notebooks/blob/master/RecEngine_NB.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Import and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.sparse as sparse\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import implicit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "df = pd.read_csv('../inputs/profile_features_clean.csv')\n",
    "menu = pd.read_csv('../raw/meals.csv')\n",
    "partyorg = pd.read_csv('../output/partyorgusers.csv')\n",
    "smalltimeinf = pd.read_csv('../output/smallinfluencerusers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "account_balance                 0\n",
       "card_brand_users             6670\n",
       "has_coupon                      0\n",
       "birthday_year               50475\n",
       "birthday_month              50332\n",
       "age                         50475\n",
       "user_id                         0\n",
       "address                         5\n",
       "zone_id                         0\n",
       "salesperson_id                  0\n",
       "discount                        0\n",
       "due_dates_only                  0\n",
       "card_details                35326\n",
       "card_brand_delivery_info    18550\n",
       "source                          0\n",
       "delivery_fee                    0\n",
       "meal_wave                       0\n",
       "surcharge_amount                0\n",
       "promo_code_used                 0\n",
       "gave_feedback                   0\n",
       "district                        5\n",
       "delivery_order_id               0\n",
       "item_id                         5\n",
       "item_type                       6\n",
       "quantity                        0\n",
       "unit_price                      5\n",
       "name                          136\n",
       "macros                        136\n",
       "ingredients                   278\n",
       "temperature                   216\n",
       "category                      225\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1324     NaN\n",
       "15223    NaN\n",
       "54305    NaN\n",
       "54306    NaN\n",
       "54307    NaN\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect rows where item_id is missing\n",
    "df[df.item_id.isnull()].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the above rows, there is no information of the item purchased hence I drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows where item_id is unknown\n",
    "df = df[df.item_id.notnull()]\n",
    "\n",
    "#Convert item_id into integer type\n",
    "df.item_id = df.item_id.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 57494 rows of transaction data relating to 368 unique items.\n"
     ]
    }
   ],
   "source": [
    "#Print number of data points \n",
    "print('We have {} rows of transaction data relating to {} unique items.'.format(df.shape[0],df.item_id.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10794</th>\n",
       "      <td>9721</td>\n",
       "      <td>202</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id  quantity\n",
       "10794     9721      202       1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define quantity for each SKU bought per user\n",
    "data = df.groupby(['user_id','item_id']).quantity.sum().reset_index()\n",
    "\n",
    "#Inspect dataframe\n",
    "data.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For items with quantity sum = 0 (perhaps order was cancelled), we convert to 1 to retain information on an original interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19 rows with quantity sum = 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#Print number of rows with quantity sum = 0\n",
    "print('There are {} rows with quantity sum = 0.'.format(data[data.quantity == 0].shape[0]))\n",
    "\n",
    "#Replace rows of quanity sum = 0 to 1\n",
    "data.quantity.loc[data.quantity==0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a sparse matrix of user_id by item_id, with values as quantity\n",
    "sparse_mat = sparse.csr_matrix((data['quantity'], (data['user_id'], data['item_id'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find sparsity of the matrix created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.89% of the matrix is sparse.\n"
     ]
    }
   ],
   "source": [
    "#Number of possible interactions in the matrix\n",
    "matrix_size = sparse_mat.shape[0]*sparse_mat.shape[1]\n",
    "\n",
    "#Num of items with interactions\n",
    "count_interactions = sparse_mat.size\n",
    "\n",
    "#Compute matrix sparsity\n",
    "sparsity = 100*(1 - (float(count_interactions)/float(matrix_size)))\n",
    "\n",
    "print('{}% of the matrix is sparse.'.format(round(sparsity,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparsity of matrix is pretty high. This would affect how well the recommender system performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Training and a Validation Set from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A training set and a validation set are created for model evaluation later on. \n",
    "\n",
    "The training set, will have a percentage of interactions masked as if the user never purchased the item (set to zero). The validation test set is a duplicate of the origin data reflecting original interaction information in binary form. The list of unique user_ids with interaction masked is recorded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trg_and_val_sets(data, pct_test = 0.2):\n",
    "    \n",
    "    #Make a copy of the original data to be the test set and store as binary preference matrix\n",
    "    test_set = data.copy()\n",
    "    test_set[test_set != 0] = 1 \n",
    "    \n",
    "    #Make a copy of the original data to be the training set. \n",
    "    training_set = data.copy() \n",
    "    \n",
    "    #Find indices in the data where an interaction exists\n",
    "    nonzero_inds = training_set.nonzero() \n",
    "    \n",
    "    #Where an interaction exists, zip user,item index into amlist\n",
    "    nonzero_pairs = list(zip(nonzero_inds[0], nonzero_inds[1]))\n",
    "    \n",
    "    #Initate random seed\n",
    "    random.seed(0)\n",
    "    \n",
    "    #Round number of samples needed to the nearest integer\n",
    "    num_samples = int(np.ceil(pct_test*len(nonzero_pairs))) \n",
    "    \n",
    "    #Sample a random number of user-item pairs without replacement\n",
    "    samples = random.sample(nonzero_pairs, num_samples) \n",
    "    \n",
    "    #Get user and item indices respectively\n",
    "    user_inds = [index[0] for index in samples] \n",
    "    item_inds = [index[1] for index in samples] \n",
    "    \n",
    "    #Mask interaction of randomly chosen user-item pairs by assigning them as zero\n",
    "    training_set[user_inds, item_inds] = 0 \n",
    "    \n",
    "    #Get rid of zeros in sparse array storage to save space\n",
    "    training_set.eliminate_zeros()\n",
    "    \n",
    "    #Output training, validation set and list of unique user_ids of rows that were altered\n",
    "    return training_set, test_set, list(set(user_inds)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call function \n",
    "train, test, users_with_altered_data = make_trg_and_val_sets(sparse_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model on training data and get recommendations to check against the validation test set to see how many of all the items recommended, were actually purchased by user (masked in training set). The higher the number, the better the recommender system.\n",
    "\n",
    "The benchmark used here is a popularity recommender where we recommend the most popular items to every user (same for all users)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:OpenBLAS detected. Its highly recommend to set the environment variable 'export OPENBLAS_NUM_THREADS=1' to disable its internal multithreading\n",
      "100%|██████████| 25.0/25 [00:00<00:00, 31.71it/s]\n"
     ]
    }
   ],
   "source": [
    "#Set parameters\n",
    "confidence_coef = 15\n",
    "factors = 2\n",
    "regularization = 0.1\n",
    "iterations = 25\n",
    "\n",
    "#Initialise model\n",
    "model = implicit.als.AlternatingLeastSquares(factors=factors, regularization=regularization, iterations=iterations)\n",
    "\n",
    "#Fit model on training data\n",
    "model.fit((train.T*confidence_coef).astype('double'))\n",
    "\n",
    "#Get user and item vectors from trained model\n",
    "user_vecs = model.user_factors\n",
    "item_vecs = model.item_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric used here is the area under the Receiver Operating Characteristic (or ROC) curve. A greater area under the curve means user actually purchased items higher up on the list of recommended items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate area under the curve for users with masked data \n",
    "def auc_score(predictions, test):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test, predictions)\n",
    "    return metrics.auc(fpr, tpr)\n",
    "\n",
    "def calc_mean_auc(training_set, altered_users, predictions, test_set):\n",
    "    \n",
    "    #Start empty list to store AUC for each user with masked data using ALS and popularity\n",
    "    als_auc = []\n",
    "    popularity_auc = []\n",
    "    \n",
    "    #Sum interactions for most popular items\n",
    "    pop_items = np.array(test_set.sum(axis = 0)).reshape(-1) \n",
    "    #Use items in prediction outcome\n",
    "    item_vecs = predictions[1]\n",
    "    \n",
    "    for user in altered_users: \n",
    "        #Get interactions from training data\n",
    "        training_row = training_set[user,:].toarray().reshape(-1)\n",
    "        \n",
    "        #Find where no interaction\n",
    "        zero_inds = np.where(training_row == 0)\n",
    "        \n",
    "        #Get predictions for user\n",
    "        user_vec = predictions[0][user,:]\n",
    "        #Get only those where interactions originally zero\n",
    "        pred = user_vec.dot(item_vecs).toarray()[0,zero_inds].reshape(-1)\n",
    "        \n",
    "        #Select interactions from als prediction for user where interactions were zero\n",
    "        actual = test_set[user,:].toarray()[0,zero_inds].reshape(-1) \n",
    "\n",
    "        #Get item popularity for chosen items\n",
    "        pop = pop_items[zero_inds] \n",
    "        \n",
    "        #Calculate AUC for user using ALS and popularity systems respectively\n",
    "        als_auc.append(auc_score(pred, actual))\n",
    "        popularity_auc.append(auc_score(pop, actual))\n",
    "    \n",
    "    #Return mean AUC rounded to two decimal places \n",
    "    return float('%.2f'%np.mean(als_auc)), float('%.2f'%np.mean(popularity_auc))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of ALS recommender: 0.88\n",
      "AUC of Popularity recommender: 0.93\n"
     ]
    }
   ],
   "source": [
    "#Call functions\n",
    "print('AUC of ALS recommender: {}'.format(calc_mean_auc(train, users_with_altered_data, \n",
    "              [sparse.csr_matrix(user_vecs), sparse.csr_matrix(item_vecs.T)], test)[0]))\n",
    "\n",
    "print('AUC of Popularity recommender: {}'.format(calc_mean_auc(train, users_with_altered_data, \n",
    "              [sparse.csr_matrix(user_vecs), sparse.csr_matrix(item_vecs.T)], test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC of the recommender system is lower than benchmark of popularity. The recommender system has a mean AUC of 0.88, while simply recommending popular items has a higher AUC of 0.93. This means that it may be more useful to recommend popular items instead of making personalised recommendations.\n",
    "\n",
    "It is worth actually conducting A/B testing on the personalised recommendations and measuring take up in the real world contexts to determine its accuracy since the scores are close.\n",
    "\n",
    "Moving forward, ratings of menu items can be collected to build a recommender system on explicit data instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model on all original data and get recommendations for users in The Party Organizer cluster for use to assist upselling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25.0/25 [00:00<00:00, 37.48it/s]\n"
     ]
    }
   ],
   "source": [
    "#Set parameters\n",
    "confidence_coef = 15\n",
    "factors = 2\n",
    "regularization = 0.1\n",
    "iterations = 25\n",
    "\n",
    "#Initialise model\n",
    "model = implicit.als.AlternatingLeastSquares(factors=factors, regularization=regularization, iterations=iterations)\n",
    "\n",
    "#Fit model\n",
    "model.fit((sparse_mat.T*confidence_coef).astype('double'))\n",
    "\n",
    "#Get user and item vectors from trained model\n",
    "user_vecs = model.user_factors\n",
    "item_vecs = model.item_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get top 10 recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user_id, sparse_mat, user_vecs, item_vecs, num_items=10):\n",
    "    \n",
    "    #Get user interactions data from sparse matrix \n",
    "    user_interactions = sparse_mat[user_id,:].toarray()\n",
    "    \n",
    "    #Add 1 to everything, so that items not purchased yet can be non-zero\n",
    "    user_interactions = user_interactions.reshape(-1) + 1\n",
    "    \n",
    "    #Make items already interacted zero\n",
    "    user_interactions[user_interactions > 1] = 0\n",
    "    \n",
    "    #Get dot product of user vector and all item vectors\n",
    "    rec_vector = user_vecs[user_id,:].dot(item_vecs.T)\n",
    "    \n",
    "    #Scale dot product result between 0 and 1\n",
    "    min_max = MinMaxScaler()\n",
    "    rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1,1))[:,0]\n",
    "    \n",
    "    #Get recommendation vector\n",
    "    recommend_vector = user_interactions * rec_vector_scaled \n",
    "    \n",
    "    #Sort into order of best recommendations\n",
    "    item_idx = np.argsort(recommend_vector)[::-1][:num_items]\n",
    "    \n",
    "    #Start empty list to store titles and scores\n",
    "    items = []\n",
    "    scores = []\n",
    "    \n",
    "    #Append recommended item name and scores tolist\n",
    "    for idx in item_idx:\n",
    "        items.append(menu.name.loc[menu.id == idx].iloc[0])\n",
    "        scores.append(recommend_vector[idx])\n",
    "    \n",
    "    #Define recommendations dataframe\n",
    "    recommendations = pd.DataFrame({'user_id':user_id, 'name': items, 'score': scores})\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve name information of recommended menu items for interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert id to integer type\n",
    "menu.id = menu.id.astype(int)\n",
    "\n",
    "#Retrieve name information of menu items involved in recommender system\n",
    "menu = menu[menu['id'].isin(data.item_id)]\n",
    "item_name = pd.DataFrame(menu[['id','name']])\n",
    "data_with_name = data.merge(item_name,how='left',left_on='item_id',right_on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendations for users in The Party Organiser cluster are exported to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get recommendations for users in The Party Organiser cluster\n",
    "rec = pd.DataFrame()\n",
    "for user in partyorg['0'].unique():\n",
    "    recommendations = recommend(user, sparse_mat, user_vecs, item_vecs)\n",
    "    rec = rec.append(recommendations[['user_id','name']],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output to csv\n",
    "rec.to_csv('../output/partyorg_recommendations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment below to view recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Print recommendations for users in The Party Organiser cluster\n",
    "# for user in partyorg['0'].unique():\n",
    "#     recommendations = recommend(user, sparse_mat, user_vecs, item_vecs)\n",
    "\n",
    "#     print( '\\n\\nTRANSACTION HISTORY FOR USER : ' + str(user) + '\\n' + '-'*80)\n",
    "#     print( data_with_name[data_with_name['user_id']==user][['name','item_id','quantity',]])\n",
    "#     print( '\\nRECOMMEND FOLLOWING ITEMS: \\n')\n",
    "#     print( recommendations['name'])\n",
    "#     print( '='*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
